# 9 jobs
# Only for 150m models, need to change max duration (1000 + max_duration)
wandb:
  group: adalayer_freeze
optimizer:
  weight_decay: 0.0
  eps: 1.0e-15
  beta_0: 0.9
  beta_1: 0.95
max_duration: 26000
scheduler:
  name: freeze_cosine_with_warmup
save_num_unsharded_checkpoints_to_keep: 1
sweep:
  - optimizer: 
      name: adalayerw # corrected novograd
      learning_rate: [1.0e-4, 3.16e-4, 1.0e-3, 3.16e-3, 1.0e-2, 3.16e-2, 1.0e-1, 3.16e-1, 1.0]
      att_correction: True
      lastlayer_correction: True
      update_last: True
      update_norm: True # can also set no_norm_training to false to remove LayerNorm trainable parameters
      t_freeze: 1000 # number of steps without training to update lr scales

run_name: olmo
seed: 0
dry_run: false
requeue: true

# UNCOMMENT THIS: Fill in custom wandb configuration
# wandb:
#   name: ${run_name}
#   entity: YOUR_ENTITY
#   project: opt-olmo
#   group: debug

max_duration: 25000 # 3.3B tokens / (256 batch * 512 context) (20x number of parameters) 
stop_at: ${max_duration}
global_train_batch_size: 256  # 524K tokens per batch
time_limit: 84600 # 23.5 hours in seconds
device_train_microbatch_size: 128 # For H100 with adam

device_eval_batch_size: 32
eval_subset_num_batches: 1000 # Eval on less batches for efficiency

optimizer:
  name: adamw
  learning_rate: 1.0e-3
  eps: 1.0e-15
  beta_0: 0.9
  beta_1: 0.95
  decouple_weight_decay: true
  weight_decay: 1.0e-5
  decay_norm_and_bias: true
  decay_embeddings: true
  metrics_log_interval: 100

scheduler:
  name: cosine_with_warmup
  t_warmup: 2500
  alpha_0: 0.1
  alpha_f: 0.1

activation_checkpointing: null
softmax_auxiliary_loss: true
fused_loss: false # causes compile issues

model:
  # NOTE: this model has 150m non-embedding params and 216m total params
  d_model: 1024
  n_heads: 16
  mlp_hidden_size: 4096
  n_layers: 12
  max_sequence_length: 512
  rope: true
  alibi: false
  activation_type: gelu 
  flash_attention: false # torch should do flash attention anyways, this would use flash_attn package
  n_kv_heads: ${model.n_heads} # standard attention
  include_bias: false
  attention_dropout: 0.0
  residual_dropout: 0.0
  embedding_dropout: 0.0
  block_type: sequential
  layer_norm_type: default
  attention_layer_norm: true
  attention_layer_norm_with_affine: true 
  layer_norm_with_affine: true 
  bias_for_layer_norm: false
  init_device: meta
  init_fn: mitchell
  weight_tying: false
  # T5 parameters:
  vocab_size: 32100  
  embedding_size: 32128 
  eos_token_id: 1
  pad_token_id: 0
  # GPT-neox parameters:
  # vocab_size: 50280
  # embedding_size: 50304
  # eos_token_id: 50279
  # pad_token_id: 1

fsdp:
  precision: mixed
  sharding_strategy: FULL_SHARD

compile:
  mode: default


precision: amp_bf16
max_grad_norm: 1.0
load_path: null

data:
  paths: # ${path.glob:<INSERT_PATH_TO_DATASET_HERE>/*.npy}
  pad_direction: right
  num_workers: 32
  drop_last: true
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  timeout: 0

tokenizer:
  identifier: t5-base
  truncate_direction: right


save_folder: ${oc.env:CHECKPOINTS_PATH}/${run_name}
save_overwrite: true
# Sharded checkpoints (best for restarts)
save_interval: 5000
save_num_checkpoints_to_keep: 1
# Unsharded checkpoints (for final storage)
save_interval_unsharded: null
# save_count_log_scale_unsharded: 10
save_num_unsharded_checkpoints_to_keep: 1


speed_monitor:
  window_size: 1


eval_interval: null
eval_count_log_scale: 10
evaluators:
  # - label: all-small-ppl-validation
  #   data:
  #     datasets:
  #       c4_val: ${path.glob:<INSERT_PATH_TO_C4_VALIDATION_FOLDER>/*.npy}
  #     drop_last: true

  # - label: piqa
  #   type: downstream

  # - label: hellaswag
  #   type: downstream

  # - label: winogrande
  #   type: downstream

  # - label: openbook_qa
  #   type: downstream

  # - label: sciq
  #   type: downstream

  # - label: arc_easy
  #   type: downstream

  # - label: copa
  #   type: downstream

  # - label: rte
  #   type: downstream

  # - label: commitment_bank
  #   type: downstream

  # - label: mrpc
  #   type: downstream

  # - label: sst2
  #   type: downstream